{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'aec_to_parallel_wrapper' object has no attribute 'single_action_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m num_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m env \u001b[38;5;241m=\u001b[39m multiwalker_v9\u001b[38;5;241m.\u001b[39mparallel_env(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m max_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_action_space\u001b[49m\u001b[38;5;241m.\u001b[39mhigh[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     state_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39msingle_observation_space\u001b[38;5;241m.\u001b[39mn          \u001b[38;5;66;03m# Discrete observation space\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'aec_to_parallel_wrapper' object has no attribute 'single_action_space'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from agilerl.components.replay_buffer import ReplayBuffer\n",
    "from agilerl.algorithms.td3 import TD3\n",
    "from pettingzoo.sisl import multiwalker_v9\n",
    "import torch\n",
    "from agilerl.utils.utils import initialPopulation\n",
    "\n",
    "\n",
    "# Create environment and Experience Replay Buffer\n",
    "num_envs = 1\n",
    "env = multiwalker_v9.parallel_env(render_mode=\"human\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Define the network configuration\n",
    "    NET_CONFIG = {\n",
    "        \"arch\": \"mlp\",  # Network architecture\n",
    "        \"h_size\": [32, 32],  # Network hidden size\n",
    "    }\n",
    "\n",
    "    INIT_HP = {\n",
    "        \"POPULATION_SIZE\": 3,\n",
    "        \"ALGO\": \"TD3\",  # Algorithm\n",
    "        \"CHANNELS_LAST\": False,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"LR\": 0.01,  # Learning rate\n",
    "        \"GAMMA\": 0.99,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 1000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 3,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "    }\n",
    "    \n",
    "    # Configure the multi-agent algo input arguments\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = False\n",
    "        INIT_HP[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "        INIT_HP[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = True\n",
    "        INIT_HP[\"MAX_ACTION\"] = None\n",
    "        INIT_HP[\"MIN_ACTION\"] = None\n",
    "    \n",
    "    # Create a population ready for evolutionary hyper-parameter optimisation\n",
    "    pop = initialPopulation(\n",
    "        INIT_HP[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        NET_CONFIG,\n",
    "        INIT_HP,\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "        device=device,\n",
    "    ) \n",
    "    \n",
    "    # Configure the multi-agent replay buffer\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "    \n",
    "    memory = []\n",
    "    for i in range(INIT_HP[\"POPULATION_SIZE\"]):\n",
    "        memory.append(ReplayBuffer(memory_size=10000, field_names=field_names, action_dim=action_dim[i]))\n",
    "\n",
    "\n",
    "# agent = TD3(state_dim=state_dim, action_dim=action_dim, one_hot=one_hot, max_action=max_action)   # Create TD3 agent\n",
    "\n",
    "state = env.reset()[0]  # Reset environment at start of episode\n",
    "while True:\n",
    "    action = agent.getAction(state)    # Get next action from agent\n",
    "    next_state, reward, done, _, _ = env.step(action)   # Act in environment\n",
    "\n",
    "    # Save experience to replay buffer\n",
    "    if channels_last:\n",
    "        memory.save2emoryVectEnvs(state, action, reward, np.moveaxis(next_state, [-1], [-3]), done)\n",
    "    else:\n",
    "        memory.save2memoryVectEnvs(state, action, reward, next_state, done)\n",
    "\n",
    "    # Learn according to learning frequency\n",
    "    if len(memory) >= agent.batch_size:\n",
    "        experiences = memory.sample(agent.batch_size) # Sample replay buffer\n",
    "        agent.learn(experiences)    # Learn according to agent's RL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'POLICY_FREQ'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m INIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAGENT_IDS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39magents\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Create a population ready for evolutionary hyper-parameter optimisation\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m pop \u001b[38;5;241m=\u001b[39m \u001b[43minitialPopulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mINIT_HP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALGO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNET_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mINIT_HP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINIT_HP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOPULATION_SIZE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Configure the multi-agent replay buffer\u001b[39;00m\n\u001b[1;32m     71\u001b[0m field_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/myenv/lib/python3.10/site-packages/agilerl/utils/utils.py:192\u001b[0m, in \u001b[0;36minitialPopulation\u001b[0;34m(algo, state_dim, action_dim, one_hot, net_config, INIT_HP, actor_network, critic_network, population_size, device, accelerator)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algo \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTD3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(population_size):\n\u001b[1;32m    180\u001b[0m         agent \u001b[38;5;241m=\u001b[39m TD3(\n\u001b[1;32m    181\u001b[0m             state_dim\u001b[38;5;241m=\u001b[39mstate_dim,\n\u001b[1;32m    182\u001b[0m             action_dim\u001b[38;5;241m=\u001b[39maction_dim,\n\u001b[1;32m    183\u001b[0m             one_hot\u001b[38;5;241m=\u001b[39mone_hot,\n\u001b[1;32m    184\u001b[0m             max_action\u001b[38;5;241m=\u001b[39mINIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX_ACTION\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    185\u001b[0m             index\u001b[38;5;241m=\u001b[39midx,\n\u001b[1;32m    186\u001b[0m             net_config\u001b[38;5;241m=\u001b[39mnet_config,\n\u001b[1;32m    187\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mINIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    188\u001b[0m             lr\u001b[38;5;241m=\u001b[39mINIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    189\u001b[0m             learn_step\u001b[38;5;241m=\u001b[39mINIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARN_STEP\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    190\u001b[0m             gamma\u001b[38;5;241m=\u001b[39mINIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGAMMA\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    191\u001b[0m             tau\u001b[38;5;241m=\u001b[39mINIT_HP[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTAU\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m--> 192\u001b[0m             policy_freq\u001b[38;5;241m=\u001b[39m\u001b[43mINIT_HP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOLICY_FREQ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    193\u001b[0m             actor_network\u001b[38;5;241m=\u001b[39mactor_network,\n\u001b[1;32m    194\u001b[0m             critic_networks\u001b[38;5;241m=\u001b[39mcritic_network,\n\u001b[1;32m    195\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    196\u001b[0m             accelerator\u001b[38;5;241m=\u001b[39maccelerator,\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m         population\u001b[38;5;241m.\u001b[39mappend(agent)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algo \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMADDPG\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'POLICY_FREQ'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from agilerl.algorithms.td3 import TD3\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.utils.utils import initialPopulation\n",
    "from pettingzoo.sisl import multiwalker_v9\n",
    "from tqdm import trange\n",
    "\n",
    "# Configure the environment\n",
    "env = multiwalker_v9.parallel_env(render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "# Configure the multi-agent algo input arguments\n",
    "try:\n",
    "    state_dim = [env.observation_space(agent).shape[0] for agent in env.agents]\n",
    "    one_hot = False\n",
    "except Exception:\n",
    "    state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "    one_hot = True\n",
    "\n",
    "try:\n",
    "    action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "    discrete_actions = False\n",
    "    max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "    min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "except Exception:\n",
    "    action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "    discrete_actions = True\n",
    "    max_action = None\n",
    "    min_action = None\n",
    "\n",
    "# Define the network configuration\n",
    "NET_CONFIG = {\n",
    "    \"arch\": \"mlp\",  # Network architecture\n",
    "    \"h_size\": [128, 128],  # Network hidden size\n",
    "}\n",
    "\n",
    "# Define the initial hyperparameters\n",
    "INIT_HP = {\n",
    "    \"POPULATION_SIZE\": 2,\n",
    "    \"ALGO\": \"TD3\",  # Algorithm\n",
    "    \"CHANNELS_LAST\": False,\n",
    "    \"BATCH_SIZE\": 64,  # Batch size\n",
    "    \"LR\": 0.001,  # Learning rate\n",
    "    \"GAMMA\": 0.95,  # Discount factor\n",
    "    \"MEMORY_SIZE\": 50000,  # Max memory buffer size\n",
    "    \"LEARN_STEP\": 50,  # Learning frequency\n",
    "    \"TAU\": 0.01,  # For soft update of target parameters\n",
    "    \"DISCRETE_ACTIONS\": discrete_actions,\n",
    "    \"MAX_ACTION\": max_action,\n",
    "    \"MIN_ACTION\": min_action,\n",
    "}\n",
    "\n",
    "# Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "INIT_HP[\"N_AGENTS\"] = env.num_agents\n",
    "INIT_HP[\"AGENT_IDS\"] = env.agents\n",
    "\n",
    "# Create a population ready for evolutionary hyper-parameter optimisation\n",
    "pop = initialPopulation(\n",
    "    INIT_HP[\"ALGO\"],\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    one_hot,\n",
    "    NET_CONFIG,\n",
    "    INIT_HP,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")\n",
    "\n",
    "# Configure the multi-agent replay buffer\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")\n",
    "\n",
    "# Training loop parameters\n",
    "max_episodes = 5  # Total episodes (default: 6000)\n",
    "max_steps = 900  # Maximum steps to take in each episode\n",
    "epsilon = 1.0  # Starting epsilon value\n",
    "eps_end = 0.1  # Final epsilon value\n",
    "eps_decay = 0.995  # Epsilon decay\n",
    "\n",
    "# Training loop\n",
    "for idx_epi in trange(max_episodes):\n",
    "    state = env.reset()  # Reset environment at start of episode\n",
    "    agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        actions = {}\n",
    "        state_dict = {agent_id: state[agent_id] for agent_id in env.agents}\n",
    "\n",
    "        for agent_idx, agent in enumerate(pop):\n",
    "            cont_actions, discrete_action = agent.getAction(\n",
    "                state_dict, epsilon, agent_mask={env.agents[agent_idx]: True}\n",
    "            )\n",
    "            action = discrete_action if agent.discrete_actions else cont_actions\n",
    "            actions.update(action)\n",
    "\n",
    "        next_state, reward, termination, truncation, info = env.step(actions)  # Act in environment\n",
    "\n",
    "        # Save experiences to replay buffer\n",
    "        memory.save2memory(state_dict, actions, reward, next_state, termination)\n",
    "\n",
    "        # Collect the reward\n",
    "        for agent_id, r in reward.items():\n",
    "            agent_reward[agent_id] += r\n",
    "\n",
    "        # Learn according to learning frequency\n",
    "        for agent_idx, agent in enumerate(pop):\n",
    "            agent_id = env.agents[agent_idx]\n",
    "            if (memory.counter % agent.learn_step == 0) and (len(memory) >= agent.batch_size):\n",
    "                experiences = memory.sample(agent.batch_size)  # Sample replay buffer\n",
    "                agent.learn(experiences)  # Learn according to agent's RL algorithm\n",
    "\n",
    "        state = next_state  # Update the state\n",
    "\n",
    "        # Stop episode if any agents have terminated\n",
    "        if any(termination.values()) or any(truncation.values()):\n",
    "            break\n",
    "\n",
    "    # Save the total episode reward\n",
    "    score = sum(agent_reward.values())\n",
    "    for agent in pop:\n",
    "        agent.scores.append(score)\n",
    "\n",
    "    # Update epsilon for exploration\n",
    "    epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "# Save the trained algorithm\n",
    "path = \"./models/TD3\"\n",
    "filename = \"TD3_trained_agent.pt\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "save_path = os.path.join(path, filename)\n",
    "elite.saveCheckpoint(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
