{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.sisl import multiwalker_v9\n",
    "\n",
    "env = multiwalker_v9.env(n_walkers=5, position_noise=1e-3, angle_noise=1e-3, forward_reward=1.0, terminate_reward=-100.0, fall_reward=-10.0, shared_reward=True,\n",
    "terminate_on_fall=True, remove_on_fall=True, terrain_length=200, max_cycles=500, render_mode='human')\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallel \n",
    "\n",
    "from pettingzoo.sisl import multiwalker_v9\n",
    "\n",
    "env = multiwalker_v9.parallel_env(render_mode=\"human\")\n",
    "observations, infos = env.reset()\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADDPG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 50/1000 [00:37<13:41,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50/1000\n",
      "Fitnesses: ['-185.65', '-233.38', '-210.20']\n",
      "100 fitness avgs: ['-185.65', '-233.38', '-210.20']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [01:14<11:41,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000\n",
      "Fitnesses: ['-230.16', '-120.35', '-192.38']\n",
      "100 fitness avgs: ['-207.91', '-176.86', '-189.01']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 150/1000 [01:50<11:44,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 150/1000\n",
      "Fitnesses: ['-305.65', '-231.07', '-215.51']\n",
      "100 fitness avgs: ['-219.79', '-203.03', '-189.74']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200/1000 [02:29<11:34,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/1000\n",
      "Fitnesses: ['-305.69', '-221.01', '-318.47']\n",
      "100 fitness avgs: ['-218.73', '-197.56', '-244.46']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 250/1000 [03:08<09:12,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250/1000\n",
      "Fitnesses: ['-314.04', '-327.99', '-82.00']\n",
      "100 fitness avgs: ['-220.86', '-240.58', '-191.38']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 300/1000 [04:06<13:30,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300/1000\n",
      "Fitnesses: ['-298.44', '-173.05', '-270.87']\n",
      "100 fitness avgs: ['-209.23', '-188.33', '-204.63']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 350/1000 [04:56<15:46,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 350/1000\n",
      "Fitnesses: ['-250.17', '-225.29', '-293.78']\n",
      "100 fitness avgs: ['-197.16', '-207.58', '-217.37']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/1000 [05:56<10:35,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400/1000\n",
      "Fitnesses: ['-280.94', '-210.81', '-104.99']\n",
      "100 fitness avgs: ['-216.75', '-207.99', '-194.76']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 450/1000 [07:15<11:35,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 450/1000\n",
      "Fitnesses: ['-273.44', '-105.15', '-296.40']\n",
      "100 fitness avgs: ['-203.50', '-184.80', '-206.05']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 500/1000 [08:33<10:06,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500/1000\n",
      "Fitnesses: ['-215.88', '-205.97', '-143.21']\n",
      "100 fitness avgs: ['-187.91', '-203.75', '-197.47']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 550/1000 [09:33<07:30,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 550/1000\n",
      "Fitnesses: ['-209.90', '-178.64', '-235.36']\n",
      "100 fitness avgs: ['-198.60', '-195.76', '-200.92']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 599/1000 [10:55<08:51,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600/1000\n",
      "Fitnesses: ['-227.91', '-280.49', '-237.36']\n",
      "100 fitness avgs: ['-198.44', '-202.82', '-199.23']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 650/1000 [11:56<08:55,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 650/1000\n",
      "Fitnesses: ['-251.90', '-158.20', '-263.39']\n",
      "100 fitness avgs: ['-202.55', '-196.07', '-203.44']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 700/1000 [13:07<18:00,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700/1000\n",
      "Fitnesses: ['-262.34', '-325.21', '-208.99']\n",
      "100 fitness avgs: ['-200.80', '-211.31', '-196.99']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 750/1000 [14:17<05:11,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 750/1000\n",
      "Fitnesses: ['-56.84', '-226.20', '-141.69']\n",
      "100 fitness avgs: ['-187.65', '-202.50', '-193.31']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 799/1000 [15:30<03:18,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800/1000\n",
      "Fitnesses: ['-345.08', '-269.09', '-346.74']\n",
      "100 fitness avgs: ['-197.49', '-192.74', '-197.59']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 849/1000 [16:29<05:04,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 850/1000\n",
      "Fitnesses: ['-243.47', '-222.57', '-223.16']\n",
      "100 fitness avgs: ['-195.72', '-194.50', '-194.53']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 900/1000 [17:42<01:39,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900/1000\n",
      "Fitnesses: ['-311.42', '-247.22', '-196.90']\n",
      "100 fitness avgs: ['-200.99', '-197.42', '-194.63']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 949/1000 [18:36<02:29,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 950/1000\n",
      "Fitnesses: ['-193.16', '-248.68', '-178.70']\n",
      "100 fitness avgs: ['-194.55', '-203.50', '-193.79']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 999/1000 [19:35<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/1000\n",
      "Fitnesses: ['-299.17', '-167.78', '-213.98']\n",
      "100 fitness avgs: ['-199.06', '-193.21', '-195.52']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [19:37<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "import torch\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.utils.utils import initialPopulation\n",
    "from tqdm import trange\n",
    "\n",
    "from pettingzoo.sisl import multiwalker_v9\n",
    "MultiAgentReplayBuffer\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define the network configuration\n",
    "    NET_CONFIG = {\n",
    "        \"arch\": \"mlp\",  # Network architecture\n",
    "        \"h_size\": [32, 32],  # Network hidden size\n",
    "    }\n",
    "\n",
    "    # Define the initial hyperparameters\n",
    "    INIT_HP = {\n",
    "        \"POPULATION_SIZE\": 3,\n",
    "        \"ALGO\": \"MADDPG\",  # Algorithm\n",
    "        \"CHANNELS_LAST\": False,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"LR\": 0.01,  # Learning rate\n",
    "        \"GAMMA\": 0.99,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 1000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 3,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "    }\n",
    "\n",
    "    # Define the multiwalker environment as a parallel environment\n",
    "    env = multiwalker_v9.parallel_env()\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the multi-agent algo input arguments\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = False\n",
    "        INIT_HP[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "        INIT_HP[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = True\n",
    "        INIT_HP[\"MAX_ACTION\"] = None\n",
    "        INIT_HP[\"MIN_ACTION\"] = None\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    INIT_HP[\"N_AGENTS\"] = env.num_agents\n",
    "    INIT_HP[\"AGENT_IDS\"] = env.agents\n",
    "\n",
    "    # Create a population ready for evolutionary hyper-parameter optimisation\n",
    "    pop = initialPopulation(\n",
    "        INIT_HP[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        NET_CONFIG,\n",
    "        INIT_HP,\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Configure the multi-agent replay buffer\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "    memory = MultiAgentReplayBuffer(\n",
    "        INIT_HP[\"MEMORY_SIZE\"],\n",
    "        field_names=field_names,\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Instantiate a tournament selection object (used for HPO)\n",
    "    tournament = TournamentSelection(\n",
    "        tournament_size=3,  # Tournament selection size\n",
    "        elitism=True,  # Elitism in tournament selection\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "        evo_step=1,\n",
    "    )  # Evaluate using last N fitness scores\n",
    "\n",
    "    # Instantiate a mutations object (used for HPO)\n",
    "    mutations = Mutations(\n",
    "        algo=INIT_HP[\"ALGO\"],\n",
    "        no_mutation=0.2,  # Probability of no mutation\n",
    "        architecture=0.2,  # Probability of architecture mutation\n",
    "        new_layer_prob=0.2,  # Probability of new layer mutation\n",
    "        parameters=0.2,  # Probability of parameter mutation\n",
    "        activation=0,  # Probability of activation function mutation\n",
    "        rl_hp=0.2,  # Probability of RL hyperparameter mutation\n",
    "        rl_hp_selection=[\n",
    "            \"lr\",\n",
    "            \"learn_step\",\n",
    "            \"batch_size\",\n",
    "        ],  # RL hyperparams selected for mutation\n",
    "        mutation_sd=0.1,  # Mutation strength\n",
    "        # Define search space for each hyperparameter\n",
    "        min_lr=0.0001,\n",
    "        max_lr=0.01,\n",
    "        min_learn_step=1,\n",
    "        max_learn_step=120,\n",
    "        min_batch_size=8,\n",
    "        max_batch_size=64,\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],  # Agent IDs\n",
    "        arch=NET_CONFIG[\"arch\"],  # MLP or CNN\n",
    "        rand_seed=1,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Define training loop parameters\n",
    "    max_episodes = 1000  # Total episodes (default: 6000)\n",
    "    max_steps = 500  # Maximum steps to take in each episode\n",
    "    epsilon = 1.0  # Starting epsilon value\n",
    "    eps_end = 0.1  # Final epsilon value\n",
    "    eps_decay = 0.995  # Epsilon decay\n",
    "    evo_epochs = 50  # Evolution frequency\n",
    "    evo_loop = 1  # Number of evaluation episodes\n",
    "    elite = pop[0]  # Assign a placeholder \"elite\" agent\n",
    "\n",
    "    # Training loop\n",
    "        # Training loop\n",
    "    for idx_epi in trange(max_episodes):\n",
    "        state = env.reset()  # Reset environment at start of episode\n",
    "        state = state[0]\n",
    "        agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            actions = {}\n",
    "            state_dict = {agent_id: state[agent_id] for agent_id in env.agents}\n",
    "            for agent_idx, agent in enumerate(pop) :\n",
    "                cont_actions, discrete_action = agent.getAction(state_dict, epsilon, agent_mask={env.agents[agent_idx]: True})\n",
    "                action = discrete_action if agent.discrete_actions else cont_actions\n",
    "                actions.update(action)\n",
    "\n",
    "            next_state, reward, termination, truncation, info = env.step(actions)  # Act in environment\n",
    "\n",
    "            # Save experiences to replay buffer\n",
    "            memory.save2memory(state_dict, actions, reward, next_state, termination)\n",
    "\n",
    "            # Collect the reward\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            batch_size = pop[0].batch_size\n",
    "            learn_step = pop[0].learn_step\n",
    "            # Learn according to learning frequency\n",
    "            if (memory.counter % learn_step == 0) and (len(memory) >= batch_size):\n",
    "                experiences = memory.sample(batch_size)  # Sample replay buffer\n",
    "                for agent_idx, agent in enumerate(pop) :\n",
    "                    agent.learn(experiences)  # Learn according to agent's RL algorithm\n",
    "\n",
    "            state = next_state  # Update the state\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(termination.values()) or any(truncation.values()):\n",
    "                break\n",
    "\n",
    "        # Save the total episode reward\n",
    "        score = sum(agent_reward.values())\n",
    "        for agent in pop:\n",
    "            agent.scores.append(score)\n",
    "\n",
    "        # Update epsilon for exploration\n",
    "        epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "        # Now evolve population if necessary\n",
    "        if (idx_epi + 1) % evo_epochs == 0:\n",
    "            # Evaluate population\n",
    "            fitnesses = [\n",
    "                agent.test(\n",
    "                    env,\n",
    "                    max_steps=max_steps,\n",
    "                    loop=evo_loop,\n",
    "                )\n",
    "                for agent in pop\n",
    "            ]\n",
    "\n",
    "            print(f\"Episode {idx_epi + 1}/{max_episodes}\")\n",
    "            print(f'Fitnesses: {[\"%.2f\" % fitness for fitness in fitnesses]}')\n",
    "            print(\n",
    "                f'100 fitness avgs: {[\"%.2f\" % np.mean(agent.fitness[-100:]) for agent in pop]}'\n",
    "            )\n",
    "\n",
    "            # Tournament selection and population mutation\n",
    "            elite, pop = tournament.select(pop)\n",
    "            pop = mutations.mutation(pop)\n",
    "\n",
    "    # Save the trained algorithm\n",
    "    path = \"./models/MADDPG\"\n",
    "    filename = \"MADDPG_trained_agent.pt\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_path = os.path.join(path, filename)\n",
    "    elite.saveCheckpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Episode: 0 ---------------\n",
      "Episodic Reward:  -308.13980266451836\n",
      "walker_0 reward: -102.71326755483945\n",
      "walker_1 reward: -102.71326755483945\n",
      "walker_2 reward: -102.71326755483945\n",
      "--------------- Episode: 1 ---------------\n",
      "Episodic Reward:  -309.0874107182026\n",
      "walker_0 reward: -103.02913690606754\n",
      "walker_1 reward: -103.02913690606754\n",
      "walker_2 reward: -103.02913690606754\n",
      "--------------- Episode: 2 ---------------\n",
      "Episodic Reward:  -307.6405560821295\n",
      "walker_0 reward: -102.5468520273765\n",
      "walker_1 reward: -102.5468520273765\n",
      "walker_2 reward: -102.5468520273765\n",
      "--------------- Episode: 3 ---------------\n",
      "Episodic Reward:  -321.2203716933727\n",
      "walker_0 reward: -107.07345723112425\n",
      "walker_1 reward: -107.07345723112425\n",
      "walker_2 reward: -107.07345723112425\n",
      "--------------- Episode: 4 ---------------\n",
      "Episodic Reward:  -310.38620471954346\n",
      "walker_0 reward: -103.46206823984781\n",
      "walker_1 reward: -103.46206823984781\n",
      "walker_2 reward: -103.46206823984781\n",
      "--------------- Episode: 5 ---------------\n",
      "Episodic Reward:  -310.3070905804634\n",
      "walker_0 reward: -103.43569686015447\n",
      "walker_1 reward: -103.43569686015447\n",
      "walker_2 reward: -103.43569686015447\n",
      "--------------- Episode: 6 ---------------\n",
      "Episodic Reward:  -316.19479478150606\n",
      "walker_0 reward: -105.3982649271687\n",
      "walker_1 reward: -105.3982649271687\n",
      "walker_2 reward: -105.3982649271687\n",
      "--------------- Episode: 7 ---------------\n",
      "Episodic Reward:  -300.4839228121564\n",
      "walker_0 reward: -100.16130760405213\n",
      "walker_1 reward: -100.16130760405213\n",
      "walker_2 reward: -100.16130760405213\n",
      "--------------- Episode: 8 ---------------\n",
      "Episodic Reward:  -309.3917213603854\n",
      "walker_0 reward: -103.13057378679514\n",
      "walker_1 reward: -103.13057378679514\n",
      "walker_2 reward: -103.13057378679514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Episode: 9 ---------------\n",
      "Episodic Reward:  -308.84262354299426\n",
      "walker_0 reward: -102.94754118099809\n",
      "walker_1 reward: -102.94754118099809\n",
      "walker_2 reward: -102.94754118099809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[swscaler @ 0x5826640] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from agilerl.algorithms.maddpg import MADDPG\n",
    "from PIL import Image, ImageDraw\n",
    "from pettingzoo.sisl import multiwalker_v9  # Import the correct environment\n",
    "\n",
    "\n",
    "# Define function to label the frame with episode number\n",
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "    text_color = (255, 255, 255) if np.mean(frame) < 128 else (0, 0, 0)\n",
    "    drawer.text((im.size[0] / 20, im.size[1] / 18), f\"Episode: {episode_num+1}\", fill=text_color)\n",
    "    return im\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configure the environment with render_mode \"rgb_array\"\n",
    "    env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n",
    "    env.reset()\n",
    "\n",
    "    # Define the observation and action dimensions based on Multiwalker environment\n",
    "    state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "    one_hot = False\n",
    "    action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "    discrete_actions = False\n",
    "    max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "    min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Instantiate an MADDPG object\n",
    "    maddpg = MADDPG(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        n_agents,\n",
    "        agent_ids,\n",
    "        max_action,\n",
    "        min_action,\n",
    "        discrete_actions,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Load the trained MADDPG model\n",
    "    path = \"./models/MADDPG/MADDPG_trained_agent.pt\"\n",
    "    maddpg.loadCheckpoint(path)\n",
    "\n",
    "    # Set testing parameters\n",
    "    episodes = 10\n",
    "    max_steps = 500\n",
    "\n",
    "    rewards = []\n",
    "    frames = []\n",
    "    indi_agent_rewards = {agent_id: [] for agent_id in agent_ids}\n",
    "\n",
    "    # Testing loop\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            # Get next action from agent\n",
    "            cont_actions, _ = maddpg.getAction(state, epsilon=0)\n",
    "            action = cont_actions  # Only continuous actions are used\n",
    "\n",
    "            # Render and save the frame for video creation\n",
    "            frame = env.render()  # Now renders an RGB array\n",
    "            if frame is not None:\n",
    "                frames.append(_label_with_episode_number(frame, episode_num=ep))\n",
    "\n",
    "            # Step in the environment\n",
    "            state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "            # Collect rewards for each agent\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Sum the total episode reward\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # End episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Store individual agent rewards\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "\n",
    "        print(\"-\" * 15, f\"Episode: {ep}\", \"-\" * 15)\n",
    "        print(\"Episodic Reward: \", rewards[-1])\n",
    "        for agent_id, reward_list in indi_agent_rewards.items():\n",
    "            print(f\"{agent_id} reward: {reward_list[-1]}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Save frames as an MP4 video\n",
    "    video_path = \"./videos/\"\n",
    "    os.makedirs(video_path, exist_ok=True)\n",
    "    with imageio.get_writer(os.path.join(video_path, \"multiwalker.mp4\"), fps=30) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(np.array(frame))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
